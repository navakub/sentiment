{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinnSentiment\n",
    "\n",
    "Neural Network implementation of sentiment analysis using **FinnSentiment** dataset from https://github.com/cynarr/sentiment-analysis/tree/master/data-raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Windows\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import essential packages for the project\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read data from a file\n",
    "def process_data(datatype, language, label):\n",
    "    '''\n",
    "        input:\n",
    "            - datatype: 'train' or 'test'\n",
    "            - language: 'en'(English) or 'fi'(Finnish)\n",
    "            - label: 'pos'(positive) or 'neg'(negative)\n",
    "        output:\n",
    "            - list of sentences\n",
    "    '''\n",
    "    filename = label + '_test.txt' if datatype=='test' else label + '.txt'\n",
    "    filepath = 'data-raw/bin/' + language + '/' + datatype + '/' + filename\n",
    "\n",
    "    with open(filepath, mode='r', encoding='utf8') as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    # -------------------------------------------------- data preprocessing  -------------------------------------------------- #\n",
    "    processed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower() # lowercase\n",
    "        sentence = sentence.replace('\\n','') # remove \\n  \n",
    "\n",
    "        words = word_tokenize(sentence) # tokenisation\n",
    "        # remove non-alphabet characters and punctuations\n",
    "        for word in words:\n",
    "            if (word in list(punctuation)) or word.isalpha()==False: \n",
    "                words.remove(word)\n",
    "                \n",
    "        # append list of words of a sentence to data\n",
    "        processed.append(words)\n",
    "    # -------------------------------------------------- data preprocessing  -------------------------------------------------- #\n",
    "\n",
    "    pol = 1 if label == 'pos' else 0 #polarity\n",
    "    processed = [(sentence, pol) for sentence in processed]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing successful!\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing: get list of words for each sentence\n",
    "train_pos = process_data('train','fi','pos')\n",
    "train_neg = process_data('train','fi','neg')\n",
    "test_pos = process_data('test','fi','pos')\n",
    "test_neg = process_data('test','fi','neg')\n",
    "\n",
    "print('preprocessing successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_pos + train_neg\n",
    "test = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "\n",
    "random.shuffle(train)\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, label_train = [e[0] for e in train], [e[1] for e in train]\n",
    "input_test, label_test = [e[0] for e in test], [e[1] for e in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 9066 unique words in the corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = set([word for e in input_train for word in e])\n",
    "vocab_size = len(corpus)\n",
    "print('there are %d unique words in the corpus' %(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    6427.000000\n",
       "mean        4.680878\n",
       "std         2.456990\n",
       "min         1.000000\n",
       "25%         3.000000\n",
       "50%         4.000000\n",
       "75%         6.000000\n",
       "max        25.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "length = [len(e) for e in input_train + input_test]\n",
    "maxlen = max(length)\n",
    "seq_len = maxlen\n",
    "\n",
    "pd.Series(length).hist()\n",
    "plt.show()\n",
    "pd.Series(length).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad / add unknown\n",
    "pad = '<PAD>'\n",
    "unk = '<UNK>'\n",
    "\n",
    "input_train_mod = list()\n",
    "for sentence in input_train:\n",
    "    sentence_padded = sentence\n",
    "    for j in range(seq_len):\n",
    "        if j >= len(sentence):\n",
    "            sentence_padded.append(pad)\n",
    "    input_train_mod.append(sentence_padded)\n",
    "\n",
    "input_test_mod= list()\n",
    "for sentence in input_test:\n",
    "    s = []\n",
    "    for j in range(seq_len):\n",
    "        if j >= len(sentence):\n",
    "            s.append(pad)\n",
    "        else:  \n",
    "            word = sentence[j]\n",
    "            if word not in corpus:\n",
    "                s.append(unk)\n",
    "            else:\n",
    "                s.append(word)\n",
    "    input_test_mod.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5774, 25)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_train), len(input_train[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = input_train_mod\n",
    "input_test = input_test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {w:i+2 for i,w in enumerate(corpus)}\n",
    "\n",
    "pad_val = 0\n",
    "unk_val = 1\n",
    "encoder[pad] = pad_val\n",
    "encoder[unk] = unk_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "ipt_tr = []\n",
    "for sentence in input_train:\n",
    "    s = []\n",
    "    for word in sentence:\n",
    "        s.append(encoder[word])\n",
    "    ipt_tr.append(s) \n",
    "\n",
    "ipt_t = []\n",
    "for sentence in input_test:\n",
    "    s = []\n",
    "    for word in sentence:\n",
    "        s.append(encoder[word])\n",
    "    ipt_t.append(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "ipt_tr = ipt_tr\n",
    "lab_tr = label_train\n",
    "\n",
    "# test\n",
    "ipt_t = ipt_t\n",
    "lab_t = label_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Dropout\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 25, 64)            580352    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 613441 (2.34 MB)\n",
      "Trainable params: 613441 (2.34 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(corpus) + 2 # <PAD> and <UNK>\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "dropout = 0.5\n",
    "\n",
    "model = Sequential() \n",
    "model.add(Input(shape=(seq_len,)))\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_len)) \n",
    "model.add(LSTM(embedding_dim)) # lstm from keras\n",
    "\n",
    "\n",
    "model.add(Dropout(dropout)) \n",
    "model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy']) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "116/116 [==============================] - 3s 15ms/step - loss: 0.6886 - accuracy: 0.4446 - val_loss: 0.6866 - val_accuracy: 0.4456\n",
      "Epoch 2/10\n",
      "116/116 [==============================] - 1s 11ms/step - loss: 0.5275 - accuracy: 0.4446 - val_loss: 0.5714 - val_accuracy: 0.4456\n",
      "Epoch 3/10\n",
      "116/116 [==============================] - 1s 12ms/step - loss: 0.2649 - accuracy: 0.4446 - val_loss: 0.6725 - val_accuracy: 0.4456\n",
      "Epoch 4/10\n",
      "116/116 [==============================] - 1s 12ms/step - loss: 0.1361 - accuracy: 0.4446 - val_loss: 0.8010 - val_accuracy: 0.4456\n",
      "Epoch 5/10\n",
      "116/116 [==============================] - 1s 11ms/step - loss: 0.0804 - accuracy: 0.4446 - val_loss: 0.8842 - val_accuracy: 0.4456\n",
      "Epoch 6/10\n",
      "116/116 [==============================] - 1s 11ms/step - loss: 0.0627 - accuracy: 0.4446 - val_loss: 1.2182 - val_accuracy: 0.4456\n",
      "Epoch 7/10\n",
      "116/116 [==============================] - 1s 11ms/step - loss: 0.0426 - accuracy: 0.4446 - val_loss: 1.2585 - val_accuracy: 0.4456\n",
      "Epoch 8/10\n",
      "116/116 [==============================] - 1s 12ms/step - loss: 0.0319 - accuracy: 0.4446 - val_loss: 1.2743 - val_accuracy: 0.4456\n",
      "Epoch 9/10\n",
      "116/116 [==============================] - 1s 12ms/step - loss: 0.0296 - accuracy: 0.4446 - val_loss: 1.2142 - val_accuracy: 0.4456\n",
      "Epoch 10/10\n",
      "116/116 [==============================] - 1s 12ms/step - loss: 0.0257 - accuracy: 0.4446 - val_loss: 1.7338 - val_accuracy: 0.4456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20e9d32dc00>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 50\n",
    "\n",
    "model.fit(ipt_tr, lab_tr, validation_data=(ipt_t, lab_t), epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. model evaluation visulisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 4ms/step - loss: 1.7338 - accuracy: 0.4456\n"
     ]
    }
   ],
   "source": [
    "model_eval = model.evaluate(ipt_t, lab_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm-finnsentiment-keras.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 testing and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for user prediction\n",
    "def user_input_processing(review):\n",
    "    to_predict = []\n",
    "\n",
    "    review_text = review\n",
    "\n",
    "    # preprocess the review\n",
    "    to_predict = []\n",
    "    for word in review.split(\" \"):\n",
    "        word = str.lower(word)\n",
    "        if word[-1] == \".\":\n",
    "            word = word[:-1]\n",
    "        if word not in corpus:\n",
    "            word = unk\n",
    "        to_predict.append(encoder[word])\n",
    "    to_predict = pad_sequences([to_predict], seq_len, padding='post')\n",
    "\n",
    "    # pol = 'positive' if model.predict(np.array(to_predict))[0] > 0.5 else 'negative'\n",
    "    \n",
    "    print(review_text, model.predict(np.array(to_predict))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 317ms/step\n",
      "Saat yhdeksän yhdestä paketista, koska olen hyvällä tuulella. [1.]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "täydellinen ruoka [1.]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Yksi viime vuosien hienoimmista elokuvista [1.]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Ennustettavaa ja huonoa. Näyttelijätyö oli kauheaa ja tarina yhteinen [1.]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "huonot palvelut [1.]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Olet niin kaunis. [1.]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    'Saat yhdeksän yhdestä paketista, koska olen hyvällä tuulella.', # You get nine out of one pack because I'm in a good mood\n",
    "    'täydellinen ruoka', # 'perfect food' \n",
    "    'Yksi viime vuosien hienoimmista elokuvista', # 'One of the finest films made in recent years.' \n",
    "    'Ennustettavaa ja huonoa. Näyttelijätyö oli kauheaa ja tarina yhteinen', # 'Predictable and bad. The acting was terrible and the story was common.'\n",
    "    'huonot palvelut', # 'bad services' \n",
    "    'Olet niin kaunis.' # 'you are so beautiful' \n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    user_input_processing(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
